{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import data_helpers\n",
    "from tensorflow.contrib import learn\n",
    "import csv\n",
    "import codecs\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data Parameters\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/target.txt\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/non_target.txt\", \"Data source for the negative data.\")\n",
    "\n",
    "# Eval Parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_string(\"checkpoint_dir\", \"\", \"Checkpoint directory from training run\")\n",
    "tf.flags.DEFINE_boolean(\"eval_train\", False, \"Evaluate on all training data\")\n",
    "\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "tf.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(codecs.open(positive_data_file, \"r\", \"utf-8\").readlines())\n",
    "    positive_examples = [[item for item in jieba.analyse.extract_tags(s,withWeight=False,topK=20,allowPOS=('n','v','nt','vn'))] for s in positive_examples]\n",
    "    negative_examples = list(codecs.open(negative_data_file, \"r\", \"utf-8\").readlines())\n",
    "    negative_examples = [[item for item in jieba.analyse.extract_tags(s, withWeight=False,topK=20,allowPOS=('n','v','nt','vn'))] for s in negative_examples]\n",
    "    \n",
    "    # Combine lists\n",
    "    x_text = positive_examples + negative_examples\n",
    "    \n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method FlagValues.append_flag_values of <absl.flags._flagvalues.FlagValues object at 0x123488d68>>\n"
     ]
    }
   ],
   "source": [
    "print(FLAGS.append_flag_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33min 18s, sys: 45.4 s, total: 34min 3s\n",
      "Wall time: 34min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_raw, y = load_data_and_labels(FLAGS.positive_data_file,FLAGS.negative_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['返佣',\n",
       " '平台',\n",
       " '个点',\n",
       " '点回',\n",
       " '回本',\n",
       " '批文',\n",
       " '模板',\n",
       " '成本低',\n",
       " '手续费',\n",
       " '金评',\n",
       " '参返',\n",
       " '亮点',\n",
       " '杠杆',\n",
       " '开户',\n",
       " '大图',\n",
       " '交易所',\n",
       " '国务院',\n",
       " '轮播',\n",
       " '推荐',\n",
       " '添加']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_raw[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = os.path.join(\"./runs/1531822384/\", \"vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./runs/1531822384/vocab\n"
     ]
    }
   ],
   "source": [
    "print(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = vocab_processor.vocabulary_._mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.argmax(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWord2Vector(filename):\n",
    "    \n",
    "    '''\n",
    "    Word2Vector_vocab – list of the words that we now have embeddings for\n",
    "    Word2Vector_embed – list of lists containing the embedding vectors\n",
    "    embedding_dict – dictionary where the words are the keys and the embeddings are the values\n",
    "    '''\n",
    "    \n",
    "    Word2Vector_vocab = []\n",
    "    Word2Vector_embed=[]\n",
    "    embedding_dict = {}\n",
    "\n",
    "    with open(filename,'r') as file:\n",
    "        for line in file.readlines():\n",
    "            row = line.strip().split(' ')\n",
    "            vocab_word = row[0]\n",
    "            Word2Vector_vocab.append(vocab_word)\n",
    "            embed_vector = [float(i) for i in row[1:]] # convert to list of float\n",
    "            embedding_dict[vocab_word]=embed_vector\n",
    "            Word2Vector_embed.append(embed_vector)\n",
    "            \n",
    "        print('Word2Vector Loaded Successfully')\n",
    "        return Word2Vector_vocab,Word2Vector_embed,embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vector Loaded Successfully\n",
      "CPU times: user 55.1 s, sys: 29.9 s, total: 1min 25s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Word2Vector_vocab,Word2Vector_embed,embedding_dict = loadWord2Vector(filename = \"./embd/sgns.sogounews.bigram-char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = vocab_processor.vocabulary_._mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict['消费']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Embedding array\n",
    "doc_vocab_size = len(vocab_processor.vocabulary_)\n",
    "\n",
    "# Extract word:id mapping from the object.\n",
    "vocab_dict = vocab_processor.vocabulary_._mapping\n",
    "\n",
    "# Sort the vocabulary dictionary on the basis of values(id).\n",
    "# Both statements perform same task.\n",
    "# sorted_vocab = sorted(vocab_dict.items(), key=operator.itemgetter(1))\n",
    "dict_as_list = sorted(vocab_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "embeddings_tmp = []\n",
    "\n",
    "for i in range(doc_vocab_size):\n",
    "    item = dict_as_list[i][0]\n",
    "    if item in Word2Vector_vocab:\n",
    "        embeddings_tmp.append(embedding_dict[item])\n",
    "    else:\n",
    "        rand_num = np.random.uniform(low=-0.2, high=0.2, size=300)\n",
    "        embeddings_tmp.append(rand_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.asarray(embeddings_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02892246, -0.12089643, -0.1180916 , ...,  0.09872085,\n",
       "        -0.09633021, -0.19369231],\n",
       "       [-0.631278  ,  0.521143  , -1.210023  , ..., -0.019455  ,\n",
       "         0.128484  ,  0.576405  ],\n",
       "       [ 0.10895765, -0.16969516,  0.04487347, ..., -0.18535981,\n",
       "         0.14603186,  0.09791655],\n",
       "       ...,\n",
       "       [-0.12706099,  0.11840921, -0.16086174, ..., -0.18615561,\n",
       "         0.0952969 ,  0.05164323],\n",
       "       [-0.12267822,  0.00447117, -0.17404047, ..., -0.02802657,\n",
       "         0.17591435, -0.11371115],\n",
       "       [-0.11995046, -0.18648953, -0.14091931, ..., -0.02468655,\n",
       "        -0.16428729, -0.18802503]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=[]\n",
    "for text in x_raw:\n",
    "    text_list.append(' '.join(text))\n",
    "x_test = np.array(list(vocab_processor.transform(text_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41, 42, 43, 22, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 17, 54, 55,\n",
       "       56, 57, 58])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/zhangnan/Rebate/runs/1531822384/checkpoints/model-27400\n",
      "Total number of test examples: 10922\n",
      "Accuracy: 0.996704\n",
      "Saving evaluation to ./prediction.csv\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = tf.train.latest_checkpoint(\"./runs/1531822384/checkpoints\")\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = batch_iter(list(x_test), FLAGS.batch_size, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))\n",
    "\n",
    "# Save the evaluation to a csv\n",
    "predictions_human_readable = np.column_stack((np.array(x_raw), all_predictions))\n",
    "out_path = os.path.join(FLAGS.checkpoint_dir, \".\", \"prediction.csv\")\n",
    "print(\"Saving evaluation to {0}\".format(out_path))\n",
    "with open(out_path, 'w') as f:\n",
    "    csv.writer(f).writerows(predictions_human_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./prediction.csv\n"
     ]
    }
   ],
   "source": [
    "print(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10922,)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
